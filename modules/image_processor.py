import os
import cv2
import numpy as np
import torch
from PIL import Image
from matplotlib import pyplot as plt
from segment_anything import sam_model_registry, SamPredictor
from rembg import remove



# --- initialize the SAM model ---
# https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb
SAM_CHECKPOINT = "weights/sam_vit_h_4b8939.pth"
MODEL_TYPE = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
sam.to(device=device)
sam_predictor = SamPredictor(sam)

U2_Net_Enable = False


def show_mask(mask, ax, random_color=False):
    """
    Visualization Mask
    :param mask:
    :param ax:
    :param random_color:
    :return:
    """
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)


def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels == 1]
    neg_points = coords[labels == 0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white',
               linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white',
               linewidth=1.25)


def resize_to_fit(image, max_width=1280, max_height=720):
    """
    Resizes an image to fit within a maximum width and height, maintaining aspect ratio.
    """
    h, w = image.shape[:2]
    scale_factor = min(max_width / w, max_height / h)

    if scale_factor >= 1:
        return image, 1.0  # No need to resize if it already fits

    new_width = int(w * scale_factor)
    new_height = int(h * scale_factor)

    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
    return resized_image, scale_factor


input_points = []
input_labels = []


def on_mouse_click_scaled(event, x, y, flags, param):
    original_image = param['original_image']
    scale_factor = param['scale_factor']

    # Convert mouse coordinates back to the original image's coordinates
    original_x = int(x / scale_factor)
    original_y = int(y / scale_factor)

    if event == cv2.EVENT_LBUTTONDOWN or event == cv2.EVENT_RBUTTONDOWN:
        if event == cv2.EVENT_LBUTTONDOWN:
            input_points.append([original_x, original_y])
            input_labels.append(1)  # Foreground point
        else:
            input_points.append([original_x, original_y])
            input_labels.append(0)  # Background point

        # Display points on the scaled image for visual feedback
        image_with_points = param['scaled_image'].copy()
        for i, (px, py) in enumerate(input_points):
            scaled_x = int(px * scale_factor)
            scaled_y = int(py * scale_factor)
            color = (0, 255, 0) if input_labels[i] == 1 else (0, 0, 255)
            cv2.circle(image_with_points, (scaled_x, scaled_y), 5, color, -1)

        cv2.imshow("Select ROI (Left: FG, Right: BG)", image_with_points)


def get_roi_from_sam(image_path: str):
    """
    使用 SAM 模型进行交互式 ROI 框选。
    左键点击添加前景点，右键点击添加背景点。
    """
    global input_points, input_labels
    input_points.clear()
    input_labels.clear()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        print(f"Error: Unable to read image from {image_path}")
        return None, None

    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    # Resize the image for display purposes only
    scaled_image, scale_factor = resize_to_fit(image_bgr)

    # feed the image to SAM
    sam_predictor.set_image(image_rgb)

    cv2.imshow("Select ROI (Left: FG, Right: BG)", scaled_image)
    cv2.setMouseCallback("Select ROI (Left: FG, Right: BG)", on_mouse_click_scaled,
                         param={'original_image': image_bgr, 'scaled_image': scaled_image,
                                'scale_factor': scale_factor})
    key = cv2.waitKey(0)
    cv2.destroyAllWindows()

    if key == ord('s') or key == ord('S'):
        if input_points:
            input_points_np = np.array(input_points)
            input_labels_np = np.array(input_labels)

            # 使用 SAM 生成掩码
            masks, scores, _ = sam_predictor.predict(
                point_coords=input_points_np,
                point_labels=input_labels_np,
                multimask_output=True,
            )

            if len(masks) == 0:
                print("No mask generated by SAM.")
                return None, None

            # find the best mask based on scores
            # id = np.argmax(masks)
            best_mask = masks[np.argmax(scores)]

            # print(best_mask.shape)

            plt.figure(figsize=(15, 15))
            plt.imshow(image_rgb)
            show_mask(best_mask, plt.gca())
            show_points(input_points_np, input_labels_np, plt.gca())
            plt.title("SAM Segmentation Result", fontsize=18)
            plt.axis('off')
            plt.show()

            # 找到掩码的最小包围框作为 ROI
            y_coords, x_coords = np.where(best_mask)
            if y_coords.size > 0 and x_coords.size > 0:
                ymin, ymax = np.min(y_coords), np.max(y_coords)
                xmin, xmax = np.min(x_coords), np.max(x_coords)
                roi_box = (xmin, ymin, xmax, ymax)

                roi_image_rgb = image_rgb[ymin:ymax, xmin:xmax]

                plt.figure(figsize=(10, 10))
                plt.imshow(roi_image_rgb)
                plt.title("ROI Cropped Result", fontsize=18)
                plt.axis('off')
                plt.show()
                return roi_box, best_mask
            else:
                print("No mask found, please try again.")
                return None, None

    return None, None


def extract_elements(image: np.ndarray, mask: np.ndarray):
    """
    Directly extracts elements from the original image using the SAM mask,
    creating a transparent background.
    """
    # 裁剪 ROI 区域
    y_coords, x_coords = np.where(mask)
    if y_coords.size == 0 or x_coords.size == 0:
        return None

    ymin, ymax = np.min(y_coords), np.max(y_coords)
    xmin, xmax = np.min(x_coords), np.max(x_coords)

    roi_image = image[ymin:ymax, xmin:xmax]
    roi_mask = mask[ymin:ymax, xmin:xmax]

    if not U2_Net_Enable:
        # Create a 4-channel RGBA image from the cropped image
        # The alpha channel is initialized to zeros (fully transparent)

        extracted_element = np.zeros(
            (roi_image.shape[0], roi_image.shape[1], 4),
            dtype=np.uint8
        )

        # Copy the RGB channels from the original cropped image
        extracted_element[:, :, :3] = roi_image

        # Set the alpha channel based on the cropped mask
        # The mask is boolean (True/False), so we convert it to uint8 (0/1) and scale to 0-255
        extracted_element[:, :, 3] = roi_mask.astype(np.uint8) * 255
    else:
        # 转换为 PIL Image
        pil_image = Image.fromarray(cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB))

        # rembg for background removal
        output_image = remove(pil_image, alpha_matting_foreground_threshold=240)
        extracted_element = cv2.cvtColor(np.array(output_image), cv2.COLOR_RGB2BGRA)


    return extracted_element


def process_image(image_path: str, output_path: str = "output.png"):
    """
    Processes a single image and saves the result.
    """
    print("Step 1: Selecting ROI using SAM...")
    roi_box, mask = get_roi_from_sam(image_path)

    if roi_box and mask is not None:
        print("ROI selected.")
        image = cv2.imread(image_path)

        print("Step 2: Extracting elements using deep learning matting model...")
        extracted_element = extract_elements(image, mask)

        if extracted_element is not None:
            cv2.imwrite(output_path, extracted_element)
            print(f"Successfully extracted and saved to {output_path}")

            if not U2_Net_Enable:
                # --- 可视化模拟透明背景 ---
                bg_image = np.full(extracted_element.shape, (128, 128, 128, 255), dtype=np.uint8) # 创建一个灰色的背景
                bgr_channels = extracted_element[:, :, :3]
                alpha_channel = extracted_element[:, :, 3]
                alpha_mask = cv2.cvtColor(alpha_channel, cv2.COLOR_GRAY2BGR) # 将 Alpha 通道转换为 3 通道掩码，用于融合
                blended_result = np.where(alpha_mask == 255, bgr_channels, bg_image[:, :, :3]) # 将前景（抠图结果）和背景融合

                scaled_result, _ = resize_to_fit(blended_result)
            else:
                scaled_result, _ = resize_to_fit(extracted_element)

            cv2.imshow("Extracted Element", scaled_result)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
    else:
        print("ROI selection failed or was cancelled.")


if __name__ == "__main__":
    input_dir = 'datasets/test'
    output_dir = 'output/'

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    file_list = os.listdir(input_dir)
    image_files = sorted([f for f in file_list if f.endswith('.png')])

    # 遍历所有图片文件
    # for filename in image_files:
    #     input_path = os.path.join(input_dir, filename)
    #
    #     # 自动生成输出文件名，例如 'drawing_0001.png' -> 'seg_drawing_0001.png'
    #     output_filename = f'seg_{filename}'
    #     output_path = os.path.join(output_dir, output_filename)
    #
    #     print(f"Processing file: {input_path}")
    #
    #     # 调用处理函数
    #     process_image(input_path, output_path)
    process_image('datasets/test/drawing_0030.png', 'output/seg_drawing_0030.png')
