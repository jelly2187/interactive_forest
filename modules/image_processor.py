import os

import cv2
import numpy as np
import torch
from PIL import Image
from matplotlib import pyplot as plt
from segment_anything import sam_model_registry, SamPredictor
from rembg import remove

# --- initialize the SAM model ---
# https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb
SAM_CHECKPOINT = "weights/sam_vit_h_4b8939.pth"
MODEL_TYPE = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
sam.to(device=device)
sam_predictor = SamPredictor(sam)


def show_mask(mask, ax, random_color=False):
    """
    Visualization Mask
    :param mask:
    :param ax:
    :param random_color:
    :return:
    """
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)


def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels == 1]
    neg_points = coords[labels == 0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white',
               linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white',
               linewidth=1.25)


def resize_to_fit(image, max_width=1280, max_height=720):
    """
    Resizes an image to fit within a maximum width and height, maintaining aspect ratio.
    """
    h, w = image.shape[:2]
    scale_factor = min(max_width / w, max_height / h)

    if scale_factor >= 1:
        return image, 1.0  # No need to resize if it already fits

    new_width = int(w * scale_factor)
    new_height = int(h * scale_factor)

    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
    return resized_image, scale_factor


input_points = []
input_labels = []


def on_mouse_click_scaled(event, x, y, flags, param):
    original_image = param['original_image']
    scale_factor = param['scale_factor']

    # Convert mouse coordinates back to the original image's coordinates
    original_x = int(x / scale_factor)
    original_y = int(y / scale_factor)

    if event == cv2.EVENT_LBUTTONDOWN or event == cv2.EVENT_RBUTTONDOWN:
        if event == cv2.EVENT_LBUTTONDOWN:
            input_points.append([original_x, original_y])
            input_labels.append(1)  # Foreground point
        else:
            input_points.append([original_x, original_y])
            input_labels.append(0)  # Background point

        # Display points on the scaled image for visual feedback
        image_with_points = param['scaled_image'].copy()
        for i, (px, py) in enumerate(input_points):
            scaled_x = int(px * scale_factor)
            scaled_y = int(py * scale_factor)
            color = (0, 255, 0) if input_labels[i] == 1 else (0, 0, 255)
            cv2.circle(image_with_points, (scaled_x, scaled_y), 5, color, -1)

        cv2.imshow("Select ROI (Left: FG, Right: BG)", image_with_points)


def get_roi_from_sam(image_path: str):
    """
    使用 SAM 模型进行交互式 ROI 框选。
    左键点击添加前景点，右键点击添加背景点。
    """
    global input_points, input_labels
    input_points.clear()
    input_labels.clear()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        print(f"Error: Unable to read image from {image_path}")
        return None, None

    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    # Resize the image for display purposes only
    scaled_image, scale_factor = resize_to_fit(image_bgr)

    # feed the image to SAM
    sam_predictor.set_image(image_rgb)

    cv2.imshow("Select ROI (Left: FG, Right: BG)", scaled_image)
    cv2.setMouseCallback("Select ROI (Left: FG, Right: BG)", on_mouse_click_scaled,
                         param={'original_image': image_bgr, 'scaled_image': scaled_image,
                                'scale_factor': scale_factor})
    key = cv2.waitKey(0)
    cv2.destroyAllWindows()

    if key == ord('s') or key == ord('S'):
        if input_points:
            input_points_np = np.array(input_points)
            input_labels_np = np.array(input_labels)

            # 使用 SAM 生成掩码
            masks, scores, _ = sam_predictor.predict(
                point_coords=input_points_np,
                point_labels=input_labels_np,
                multimask_output=True,
            )

            if len(masks) == 0:
                print("No mask generated by SAM.")
                return None, None

            # find the best mask based on scores
            # id = np.argmax(masks)
            best_mask = masks[np.argmax(scores)]

            # print(best_mask.shape)

            plt.figure(figsize=(15, 15))
            plt.imshow(image_rgb)
            show_mask(best_mask, plt.gca())
            show_points(input_points_np, input_labels_np, plt.gca())
            plt.title("SAM Segmentation Result", fontsize=18)
            plt.axis('off')
            plt.show()

            # --- Fix: Resize the mask to match the original image dimensions ---
            # The original image dimensions are image_rgb.shape[1] (width) and image_rgb.shape[0] (height)
            # mask_resized = cv2.resize(best_mask.astype(np.uint8),
            #                           (image_rgb.shape[1], image_rgb.shape[0]),
            #                           interpolation=cv2.INTER_NEAREST)
            # mask_resized = mask_resized.astype(bool)
            #
            # # --- Use the resized mask for visualization ---
            # mask_image = np.zeros_like(image_rgb, dtype=np.uint8)
            # mask_image[mask_resized] = [0, 255, 0]  # Use the resized mask
            #
            # # 创建一个半透明的覆盖层
            # alpha = 0.5
            # visualized_result = cv2.addWeighted(image_rgb, 1 - alpha, mask_image, alpha, 0)
            #
            # # 在可视化图像上绘制输入点
            # for i, (px, py) in enumerate(input_points):
            #     color = (0, 255, 0) if input_labels[i] == 1 else (0, 0, 255)
            #     cv2.circle(visualized_result, (px, py), 5, color, -1)
            #
            # cv2.imshow("SAM Segmentation Result", cv2.cvtColor(visualized_result, cv2.COLOR_RGB2BGR))
            # cv2.waitKey(0)
            # cv2.destroyAllWindows()
            # --- 可视化部分结束 ---

            # 找到掩码的最小包围框作为 ROI
            y_coords, x_coords = np.where(best_mask)
            if y_coords.size > 0 and x_coords.size > 0:
                ymin, ymax = np.min(y_coords), np.max(y_coords)
                xmin, xmax = np.min(x_coords), np.max(x_coords)
                roi_box = (xmin, ymin, xmax, ymax)

                roi_image_rgb = image_rgb[ymin:ymax, xmin:xmax]

                plt.figure(figsize=(10, 10))
                plt.imshow(roi_image_rgb)
                plt.title("ROI Cropped Result", fontsize=18)
                plt.axis('off')
                plt.show()
                return roi_box, best_mask
            else:
                print("No mask found, please try again.")
                return None, None

    return None, None


def extract_elements(image: np.ndarray, mask: np.ndarray):
    """
    Directly extracts elements from the original image using the SAM mask,
    creating a transparent background.
    """
    # 裁剪 ROI 区域
    y_coords, x_coords = np.where(mask)
    if y_coords.size == 0 or x_coords.size == 0:
        return None

    ymin, ymax = np.min(y_coords), np.max(y_coords)
    xmin, xmax = np.min(x_coords), np.max(x_coords)

    roi_image = image[ymin:ymax, xmin:xmax]
    roi_mask = mask[ymin:ymax, xmin:xmax]

    # Create a 4-channel RGBA image from the cropped image
    # The alpha channel is initialized to zeros (fully transparent)
    extracted_element = np.zeros(
        (roi_image.shape[0], roi_image.shape[1], 4),
        dtype=np.uint8
    )

    # Copy the RGB channels from the original cropped image
    extracted_element[:, :, :3] = roi_image

    # Set the alpha channel based on the cropped mask
    # The mask is boolean (True/False), so we convert it to uint8 (0/1) and scale to 0-255
    extracted_element[:, :, 3] = roi_mask.astype(np.uint8) * 255

    # # 转换为 PIL Image
    # pil_image = Image.fromarray(cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB))
    #
    # # rembg for background removal
    # output_image = remove(pil_image, alpha_matting_foreground_threshold=200)
    # extracted_element = cv2.cvtColor(np.array(output_image), cv2.COLOR_RGB2BGRA)


    # # 使用抠图模型进行处理
    # inputs = matting_processor(images=pil_image, return_tensors="pt").to(device)
    # with torch.no_grad():
    #     outputs = matting_model(**inputs)

    # alpha_channel = outputs.alphas
    # # 将 alpha 通道从张量转换为 numpy 数组并调整大小
    # alpha_channel_np = alpha_channel.squeeze().cpu().numpy()
    # alpha_channel_np = (alpha_channel_np * 255).astype(np.uint8)
    #
    # # 创建一个 4 通道的图像 (RGB + Alpha)
    # extracted_element = cv2.cvtColor(roi_image, cv2.COLOR_BGR2BGRA)
    # extracted_element[:, :, 3] = alpha_channel_np

    return extracted_element


def process_image(image_path: str, output_path: str = "output.png"):
    """
    Processes a single image and saves the result.
    """
    print("Step 1: Selecting ROI using SAM...")
    roi_box, mask = get_roi_from_sam(image_path)

    if roi_box and mask is not None:
        print("ROI selected.")
        image = cv2.imread(image_path)

        print("Step 2: Extracting elements using deep learning matting model...")
        extracted_element = extract_elements(image, mask)

        if extracted_element is not None:
            cv2.imwrite(output_path, extracted_element)
            print(f"Successfully extracted and saved to {output_path}")

            # 可视化结果
            cv2.namedWindow("Extracted Element", cv2.WINDOW_NORMAL)
            cv2.imshow("Extracted Element", extracted_element)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
    else:
        print("ROI selection failed or was cancelled.")


if __name__ == "__main__":
    input_dir = 'datasets/test'
    output_dir = 'output/'

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    file_list = os.listdir(input_dir)
    image_files = sorted([f for f in file_list if f.endswith('.png')])

    # 遍历所有图片文件
    # for filename in image_files:
    #     input_path = os.path.join(input_dir, filename)
    #
    #     # 自动生成输出文件名，例如 'drawing_0001.png' -> 'seg_drawing_0001.png'
    #     output_filename = f'seg_{filename}'
    #     output_path = os.path.join(output_dir, output_filename)
    #
    #     print(f"Processing file: {input_path}")
    #
    #     # 调用处理函数
    #     process_image(input_path, output_path)
    process_image('datasets/test/drawing_0010.png', 'output/seg_drawing_0010.png')
